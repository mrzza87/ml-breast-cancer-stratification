{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a719fb5",
   "metadata": {},
   "source": [
    "# 🧬 Breast Cancer Subtype Classification using Deep Learning (METABRIC Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631cd47",
   "metadata": {},
   "source": [
    "\n",
    "This notebook uses the METABRIC breast cancer dataset from Kaggle to build a deep learning model that classifies cancer subtypes based on gene expression profiles.\n",
    "\n",
    "We will go through:\n",
    "- Loading and preprocessing the data\n",
    "- Building a deep neural network (DNN)\n",
    "- Training and evaluating the model\n",
    "- Visualizing the performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd810e9",
   "metadata": {},
   "source": [
    "## 📥 Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fe6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/jvzb90d54yjbgpt08dkr77s80000gn/T/ipykernel_18096/1626829915.py:6: DtypeWarning: Columns (678,688,690,692) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"METABRIC_RNA_Mutation.csv\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Living'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/wt/jvzb90d54yjbgpt08dkr77s80000gn/T/ipykernel_18096/1626829915.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m y = df[\u001b[33m\"subtype_encoded\"\u001b[39m]\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Scale the gene expression data\u001b[39;00m\n\u001b[32m     21\u001b[39m scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m X_scaled = scaler.fit_transform(X)\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n\u001b[32m     25\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=\u001b[32m0.2\u001b[39m, stratify=y, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/utils/_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m     @wraps(f)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    141\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    142\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    143\u001b[39m             return_tuple = (\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    914\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    916\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    918\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    833\u001b[39m             Fitted scaler.\n\u001b[32m    834\u001b[39m         \"\"\"\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    836\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1147\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1148\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1149\u001b[39m                 )\n\u001b[32m   1150\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    869\u001b[39m         self : object\n\u001b[32m    870\u001b[39m             Fitted scaler.\n\u001b[32m    871\u001b[39m         \"\"\"\n\u001b[32m    872\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         X = self._validate_data(\n\u001b[32m    874\u001b[39m             X,\n\u001b[32m    875\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n\u001b[32m    876\u001b[39m             dtype=FLOAT_DTYPES,\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    600\u001b[39m                 out = y\n\u001b[32m    601\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    602\u001b[39m                 out = X, y\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m             out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m    605\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m    606\u001b[39m             out = _check_y(y, **check_params)\n\u001b[32m    607\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    914\u001b[39m                         )\n\u001b[32m    915\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    916\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m    919\u001b[39m                 raise ValueError(\n\u001b[32m    920\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m    921\u001b[39m                 ) from complex_warning\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp)\u001b[39m\n\u001b[32m    376\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    377\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    378\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    379\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    381\u001b[39m \n\u001b[32m    382\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    383\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~/miniforge3/envs/ml/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   2082\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __array__(self, dtype: npt.DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> np.ndarray:\n\u001b[32m   2083\u001b[39m         values = self._values\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m         arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2085\u001b[39m         if (\n\u001b[32m   2086\u001b[39m             astype_is_view(values.dtype, arr.dtype)\n\u001b[32m   2087\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m using_copy_on_write()\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Living'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"METABRIC_RNA_Mutation.csv\")\n",
    "\n",
    "# Drop rows with missing subtype labels\n",
    "df = df.dropna(subset=[\"pam50_+_claudin-low_subtype\"])\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "df[\"subtype_encoded\"] = le.fit_transform(df[\"pam50_+_claudin-low_subtype\"])\n",
    "\n",
    "# Select gene expression features (drop non-feature columns)\n",
    "non_feature_cols = df.columns[:30].tolist() + [\"pam50_+_claudin-low_subtype\", \"subtype_encoded\"]\n",
    "X = df.drop(columns=non_feature_cols)\n",
    "y = df[\"subtype_encoded\"]\n",
    "\n",
    "# Scale the gene expression data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4fc3e",
   "metadata": {},
   "source": [
    "## 🤖 Build & Train Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fccc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode target\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_cat, validation_split=0.2, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302fc36",
   "metadata": {},
   "source": [
    "## 📏 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c07a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f6b0b",
   "metadata": {},
   "source": [
    "## 📊 Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cbc7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "conf = confusion_matrix(y_test, y_pred_classes)\n",
    "sns.heatmap(conf, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e60a2",
   "metadata": {},
   "source": [
    "## 💾 Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42109a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "model.save(\"metabric_subtype_classifier.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
